# Variational Autoencoders for Multi-Modal Music Clustering: A Comprehensive Literature Review

Variational Autoencoders (VAEs) have emerged as powerful architectures for learning interpretable music representations, though the field has evolved rapidly toward diffusion models and self-supervised approaches. This review synthesizes **60+ papers** from top venues (NeurIPS, ICML, ICLR, ISMIR, ICASSP) spanning 2017-2025, providing the technical foundation for implementing VAE-based hybrid-language music clustering systems. Key findings indicate that hierarchical VAE architectures effectively capture musical structure, multi-modal fusion of audio and lyrics achieves **94.58% accuracy** on emotion classification (Pyrovolakis et al., 2022), and Gaussian Mixture VAE priors naturally align with multi-genre music distributions—making them particularly suited for clustering tasks.

## VAE architectures for music reveal a hierarchy of capabilities

The foundational **MusicVAE** (Roberts et al., ICML 2018) introduced hierarchical recurrent VAEs with a bidirectional LSTM encoder and "conductor" decoder that generates subsequence embeddings before independent decoding. This architecture directly addresses the **posterior collapse problem** endemic to recurrent VAEs by forcing the decoder to rely on latent codes. Trained on the Lakh MIDI Dataset (170,000+ sequences), MusicVAE enables smooth latent space interpolation across 16-bar sequences. Code is available at github.com/magenta/magenta.

**Convolutional VAE architectures** for audio spectrograms gained traction through NSynth (Engel et al., ICML 2017), which employs WaveNet-style autoencoders with 30 dilated convolution layers, achieving 32× compression to 16-dimensional temporal embeddings. The accompanying NSynth dataset (**305,979 musical notes** from 1,006 instruments) remains a benchmark for timbre analysis. However, computational costs are substantial—training required 10 days on 32 K40 GPUs.

**VQ-VAE variants** dominate high-fidelity audio generation. Jukebox (Dhariwal et al., OpenAI 2020) introduced multi-scale hierarchical VQ-VAE with three compression levels (8×, 32×, 128×), generating minute-long audio conditioned on artist, genre, and lyrics using sparse transformers. Despite its **5 billion parameters** and ability to produce coherent singing, inference remains impractical (~9 hours per minute of audio). The VQ-VAE paradigm underlies modern audio codecs (EnCodec, DAC) powering subsequent systems like MusicGen.

| Architecture | Key Paper | Domain | Innovation | Dataset | Code |
|-------------|-----------|--------|------------|---------|------|
| Hierarchical RNN-VAE | MusicVAE (Roberts et al., ICML 2018) | Symbolic | Conductor decoder solves posterior collapse | Lakh MIDI | github.com/magenta/magenta |
| WaveNet Autoencoder | NSynth (Engel et al., ICML 2017) | Audio | Temporal embeddings for timbre morphing | NSynth (305K) | github.com/magenta/nsynth |
| Multi-scale VQ-VAE | Jukebox (Dhariwal et al., 2020) | Audio | 3-level discrete codes, lyrics conditioning | 1.2M songs | github.com/openai/jukebox |
| Gaussian Mixture VAE | GM-VAE (Luo et al., ISMIR 2019) | Audio | Timbre/pitch disentanglement | Instrument sounds | — |
| Tree-structured VAE | PianoTree VAE (Wang et al., ISMIR 2020) | Symbolic | Beat-note hierarchical structure | Polyphonic piano | — |
| Transformer VAE | MuseMorphose (Wu & Yang, 2021) | Symbolic | Bar-level style transfer | Pop909 | — |

**Conditional VAEs (CVAEs)** enable attribute-controlled generation. MIDI-Sandwich (Liang et al., 2019) combines hierarchical CVAEs (L-CVAE for bars, G-VAE for song structure) with adversarial components, generating 17×8 beat sequences conditioned on musical form and tonic. Emotion-conditioned CVAEs (IEEE Access 2021) produce monophonic melodies matching specified emotional valence.

**Beta-VAE and disentanglement approaches** separate musical factors. The GM-VAE (Luo et al., ISMIR 2019) uses Gaussian mixture priors with separate encoders for pitch and timbre, enabling many-to-many instrument transfer. Tanaka et al. (ICASSP 2021) improved disentanglement through VAE-based metric learning with contrastive losses. GLSR-VAE (Hadjeres et al., IEEE SSCI 2017) introduced geodesic latent space regularization, binding latent dimensions to interpretable musical attributes.

## Multi-modal fusion techniques show complementary audio-lyrics benefits

Research consistently demonstrates that combining audio and lyrics outperforms unimodal approaches. Liu and Tan (2020) achieved **79.2% accuracy** with multimodal fusion versus 70.6% (audio-only) and 62.9% (lyrics-only). The highest reported accuracy of **94.58%** (Pyrovolakis et al., Sensors 2022) uses late fusion with CNN-processed mel-spectrograms and BERT-encoded lyrics via stacking ensemble.

**Audio feature extraction** has evolved from hand-crafted to learned representations:

- **Traditional features**: MFCCs (13-40 coefficients), mel-spectrograms (typically 128 bands, n_fft=2048, hop_length=512), chromagrams (12 pitch classes), spectral centroid/rolloff
- **Learned embeddings**: VGGish (128-dim, AudioSet-trained), OpenL3 (512/6144-dim, self-supervised), PANNs/CNN14 (768-dim, state-of-the-art on AudioSet), PaSST (Vision Transformer, 768-dim)

OpenL3 (Cramer et al., ICASSP 2019) deserves special mention for music applications—trained via audio-visual correspondence on AudioSet, with a music-specific model available at github.com/marl/openl3.

**Lyrics embedding approaches** have shifted from sparse representations to contextual transformers:

- **Traditional**: TF-IDF, Word2Vec, GloVe (static, limited semantics)
- **Modern**: BERT/RoBERTa (92% accuracy on lyrics emotion classification via LyEmoBERT), XLNet (handles longer context than BERT's 512-token limit), sentence transformers (all-MiniLM-L6-v2 for retrieval)

**Fusion strategies** exhibit distinct trade-offs:

**Early fusion** (feature concatenation) captures all cross-modal interactions but faces dimensionality challenges. Laurier et al. (ICMLA 2008) pioneered this approach, concatenating MFCCs with LSA-processed lyrics achieving 92.40% accuracy via SVM.

**Late fusion** (decision-level) enables modality-specific optimization. Pyrovolakis et al. (2022) achieved state-of-the-art by training CNN and BERT encoders independently, then combining via stacking—robust to missing modalities.

**Cross-modal attention** captures fine-grained interactions. BART-fusion (Zhang et al., ISMIR 2022) injects audio representations into a pretrained language model via cross-modal attention, achieving SOTA on lyric interpretation (ROUGE, METEOR, BERT-Score metrics). Zhao et al. (2022) demonstrated that 8-head cross-modal attention hierarchically fusing CNN audio, BERT lyrics, and ALBERT metadata achieves R²=0.306 on valence prediction.

**Contrastive audio-text learning** represents the current frontier. MuLan (Huang et al., ISMIR 2022) trained a two-tower model (ResNet-50 audio + BERT text) on **44 million recordings** with weakly-associated text, enabling zero-shot music tagging via natural language queries. CLAP (Elizalde et al., ICASSP 2023) provides open-source audio-text embeddings (github.com/LAION-AI/CLAP) achieving **89.98% zero-shot accuracy** on ESC-50. LAION-Audio-630K accompanies this release with 633,526 audio-text pairs.

## Deep clustering methods for music remain underexplored

**Variational Deep Embedding (VaDE)** (Jiang et al., IJCAI 2017) provides the theoretical foundation for VAE-based clustering, using GMM priors where cluster selection precedes latent sampling before decoding. On standard benchmarks (MNIST: 94.46% accuracy), VaDE outperforms two-stage approaches. Code: github.com/slim1017/VaDE.

**Deep Embedded Clustering (DEC)** (Xie et al., ICML 2016) uses stacked autoencoder pretraining followed by fine-tuning with KL divergence clustering loss. **IDEC** (Guo et al., IJCAI 2017) improves upon DEC by jointly optimizing clustering and reconstruction objectives, preserving local structure.

**Music-specific clustering applications** remain sparse. Most VAE papers (MusicVAE, Jukebox) focus on generation rather than clustering. CLMR (Spijkervet & Burgoyne, ISMIR 2021) adapted SimCLR for music with audio-specific augmentations (pitch shift, time stretch), achieving **33.1% AP with only 1% labeled data** on MagnaTagATune—demonstrating that contrastive representations cluster meaningfully even without explicit clustering objectives. Code: github.com/Spijkervet/CLMR.

**Evaluation metrics** for music clustering should include:

- **Internal (no labels)**: Silhouette Score (cohesion vs. separation, [-1,1]), Calinski-Harabasz Index (higher=better), Davies-Bouldin Index (lower=better)
- **External (with labels)**: Adjusted Rand Index (ARI, chance-adjusted), Normalized Mutual Information (NMI), Cluster Purity, Hungarian-matched Accuracy

Best practices from VaDE literature suggest reporting multiple metrics, comparing against PCA+K-means and autoencoder+K-means baselines, and visualizing latent spaces via t-SNE/UMAP with cluster/genre coloring.

## Multi-modal and multilingual datasets enable diverse research

**For audio-lyrics research**, three datasets stand out:

**WASABI** (Buffa et al., ESWC 2021) contains **2+ million songs** with 1.73M lyrics including structural annotations (verse/chorus segmentation), emotion tags from Last.fm, and topic modeling. Available under CC BY-NC-SA via GitHub (micbuffa/WasabiDataset) with REST API and SPARQL endpoint.

**DALI** (Meseguer-Brocal et al., TISMIR 2020) provides **7,756 songs** with word-level and note-level time-aligned lyrics at four granularity levels. Approximately 80% English, with multilingual content. Audio retrieved via YouTube links. GitHub: gabolsgabs/DALI.

**4MuLA** (Silva et al., WebMedia 2020) focuses on Latin music with **96,458 songs** across 76 genres, providing both audio and lyrics features for multilingual text analysis.

**Standard MIR benchmarks** include:

- **GTZAN**: 1,000 clips (30s), 10 genres—use fault-filtered splits from Kereliuk & Sturm due to known mislabeling
- **MTG-Jamendo** (Bogdanov et al., ICML Workshop 2019): **55,525 full tracks** with 195 tags (genre/instrument/mood), CC-licensed, pre-computed mel-spectrograms
- **FMA** (Defferrard et al., ISMIR 2017): **106,574 tracks** with librosa/essentia features, hierarchical genre taxonomy
- **MagnaTagATune**: 25,863 clips with 188 tags—primary benchmark for music tagging

**Multilingual resources** for non-English music include:

| Dataset | Languages | Size | Content |
|---------|-----------|------|---------|
| MIR-MLPop (Wang et al., ICASSP 2024) | Mandarin, Cantonese, Hokkien | 90 tracks | Time-aligned lyrics (first for Cantonese/Hokkien) |
| SingStyle111 (Dai et al., ISMIR 2024) | English, Chinese, Italian | 111 songs, 224 interpretations | Phoneme-level alignment, style annotations |
| BanglaMusicStylo (Hossain & Al Marouf, ICBSLP 2018) | Bangla | 2,824 lyrics, 211 lyricists | Stylometric analysis (lyrics only, no audio) |
| MAVL | EN/ES/FR/JP/KR | 228 songs | Animated musical translations |

**Preprocessing best practices** from literature:

- **Audio**: 22.05kHz sampling (music), mel-spectrograms with 128 bands, artist-stratified train/test splits to prevent data leakage
- **Lyrics**: Remove structural markers ([Chorus], [Verse]), handle contractions, language-specific tokenization (BPE for multilingual)
- **Augmentation**: SpecAugment for spectrograms, pitch shift/time stretch for audio, back-translation for lyrics

## Recent advances show paradigm shifts away from pure VAEs

The 2020-2025 period reveals fundamental shifts: **diffusion models** have largely replaced VAEs for generation quality, while **self-supervised learning** dominates representation learning.

**MERT** (Li et al., ICLR 2024) represents the state-of-the-art for music understanding. This BERT-style transformer (95M-330M parameters) uses dual teachers—RVQ-VAE for acoustic information and CQT for musical structure. Trained via masked language modeling on ~1,000 hours, MERT achieves **SOTA on 14 MIR tasks** including genre, emotion, and key detection. Ablations confirm the CQT musical teacher is crucial for pitch-related tasks. Code: github.com/yizhilll/MERT; HuggingFace: m-a-p/MERT-v1-330M.

**MuQ** (Tencent, 2025) extends this paradigm with iterative residual vector quantization training, achieving **new SOTA with 100× less data** (0.9K hours vs. MERT's 1K). MuQ-MuLan achieves 79.3 ROC-AUC on MagnaTagATune zero-shot tagging. Code: github.com/tencent-ailab/MuQ.

**For generation**, AudioLDM 2 (Liu et al., IEEE TASLP 2024) combines GPT-2, latent diffusion, and AudioMAE for unified audio generation. The "Language of Audio" representation enables SOTA text-to-music generation, outperforming MusicGen by 3.4% on FAD metrics. Code: github.com/haoheliu/AudioLDM2.

**MusicGen** (Copet et al., NeurIPS 2023) demonstrates that single-stage transformer language models with efficient codebook interleaving match cascaded model quality while enabling text and melody conditioning. Code: github.com/facebookresearch/audiocraft.

**Key benchmark results on MagnaTagATune** (music tagging):

| Model | ROC-AUC | Type | Data Required |
|-------|---------|------|---------------|
| Short-chunk CNN + Res | 91.2 | Supervised | Full labels |
| MERT-330M | 91.0 | Self-supervised | No labels |
| CLMR | ~90.0 | Self-supervised | No labels |
| MuQ-MuLan (zero-shot) | 79.3 | Zero-shot | No labels |

## Research gaps suggest promising directions for VAE-based clustering

Several underexplored areas emerge from this review:

**VAE-specific clustering for music** lacks dedicated investigation. While VaDE demonstrates strong clustering on image/text benchmarks, direct application to music latent spaces with GMM priors aligned to genre/mood distributions remains unexplored. The natural multi-modality of musical genres suggests GMM-VAE approaches may be particularly effective.

**Multi-modal clustering** combining audio and lyrics for unsupervised music organization has no published benchmarks. Existing multi-modal work focuses on supervised classification rather than discovering latent structure.

**Multilingual music clustering** represents a near-complete gap. BanglaMusicStylo provides Bangla lyrics but no audio; MIR-MLPop covers Asian languages but at small scale (90 tracks). A hybrid-language clustering system would address real-world streaming scenarios.

**Long-form structure** remains challenging—most models operate on 16-32 bar segments without capturing verse-chorus coherence.

**Evaluation standardization** for clustering meaningfulness (beyond genre accuracy) requires development. Musical meaningfulness may not align with genre labels.

## Conclusion

This review identifies a clear technical pathway for VAE-based multi-modal music clustering: hierarchical VAE architectures (MusicVAE-style) or VaDE with GMM priors provide principled latent spaces; cross-modal attention fusion (BART-fusion style) or contrastive learning (CLAP/MuLan) effectively combines audio and lyrics; WASABI and DALI offer suitable multi-modal data; and emerging self-supervised methods (MERT, MuQ) provide strong pretrained representations for initialization. The most impactful contribution would combine these elements with explicit clustering objectives on hybrid-language data—an unexplored configuration with clear practical applications for music organization and recommendation systems. For NeurIPS-style submissions, emphasizing the clustering-generation connection (using GMM latent priors for both organization and conditional generation) and demonstrating on multilingual data would address active research gaps.